---
title: "DataExploration"
author: "Adante Henderson"
date: "2022-08-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Required Libraries

Below we Load the Required Libraries:

```{r}
library(purrr)
library(tidyverse)
library(lubridate)
library(sys)
library(fixest)
```

## Data Preperation

Initial Data Prep

As the first step of our data preperation process we combine all our google trends data extracts. We will do some initial file cleaning, dropping NA values for rows that did not return a google trends value, coverting the string date value into a date field, adding a baseline month value for rollup purposes and standardizing the indices between schools for the purpose of aggregation and comparison.

```{r}
filesnames <- list.files("../data/googletrends", full.names = TRUE)

google_data <- map_df(filesnames, read_csv) %>%
drop_na(index) %>%
mutate(dateranked = ymd(str_sub(monthorweek, end = 10))) %>%
mutate(datemonth = floor_date(dateranked, unit = "month")) %>%
group_by(keyword) %>% mutate(standardindexkeyword = ((index - mean(index))/sd(index))) %>%
group_by(schname) %>% mutate(standardindexschool = ((index - mean(index))/sd(index)))


```

Data Joining

The second step of the data preparation process is reading in our scorecard data and the linking table that will allow us to connect our scorecard data to our Google trends data. We then link the two files using inner joins. As apart of this process we filter out any schools that share the exact same name to simplify our analysis.

```{r}
scorecarddata <- read_csv("../data/Most+Recent+Cohorts+(Scorecard+Elements).csv") %>%
  rename(unitid = UNITID)

id_name_link <- read_csv("../data/id_name_link.csv") %>%
group_by(schname) %>% mutate(n= n()) %>% filter( n == 1)

joineddata <- inner_join(id_name_link, google_data, by = "schname")
joineddata <- inner_join(scorecarddata, joineddata, by = "unitid")

 
```

Final Data Cleanup

Once our complete dataset is available I limit the data set to the key exploratory variables I am interested in as well as some addition possible flags of interest. The key variables I will be investigated are cost of attendance, average student earnings, whether the scorecard was in effect and any seasonality. In order to prepare these analysis I set up several variables. 

First off I characterize high earning universities as universities in which graduates after 10 years made 1 standard deviation or higher more than the average income of all graduates after 10 years. I wanted to use 1 standard deviation above specifically to ensure that we were able to focus on the effect of "high" earning potential and filtering out schools that were just slightly above average that I did not feel would drive behavior from income motivated students. This is something that felt more confident in after performing initial analysis of the data set and finding that the mean student income in 2015 was only around 40,000 when taking into account all universities in the scorecard. That is frankly not what I would consider an impressive salary in an entry level position much less for someone 10 years into their career. It is my assumption that because the data set is country-wide the average salary number is biased lower due to the large amount of rural lower-income areas throughout the continental US. This is something I am a bit irked about as region was one of the variables I had specifically wanted to investigate when I looked over the college scorecard data dictionary but found missing in the actual pull. In addition to high earning I created a variable to identify high cost schools (using the same criteria), as well as low cost and low earning schools in order to test the opposite side of the effect. In addition I created a binary variable that was true after the release of the scorecard and false prior to the release to track whether the treatment was applied, and I created a month value in to use to control for seasonality later in my analysis.

Within this step I also filtered to only predominately bachelor providing universities, and filtered out any schools which either did not provide average cost of attendance or average student earnings.

```{r}

joineddata <- joineddata %>% select(schid, schname, keyword, index,standardindexkeyword, standardindexschool, dateranked, datemonth, PREDDEG,HBCU,RELAFFIL,'md_earn_wne_p10-REPORTED-EARNINGS', 'NPT4_PUB-AVERAGE-ANNUAL-COST',NPT4_PRIV)%>%
  rename(avgstudearnings = 'md_earn_wne_p10-REPORTED-EARNINGS', avgpubcost = 'NPT4_PUB-AVERAGE-ANNUAL-COST')

joineddata <- joineddata %>% 
  mutate(costofattend = case_when(avgpubcost == 'NULL' ~ NPT4_PRIV,TRUE ~ avgpubcost)) %>%
  filter(costofattend != 'NULL') %>%
  filter(PREDDEG == '3') %>%
  filter(avgstudearnings != 'NULL' & avgstudearnings != 'PrivacySuppressed')%>%
  mutate(costofattend = as.numeric(costofattend), avgstudearnings = as.numeric(avgstudearnings))%>%
  mutate(highearning = avgstudearnings > (mean(avgstudearnings) + sd(avgstudearnings)), highcost = costofattend > (mean(costofattend) + sd(costofattend)))%>%
  mutate(lowearning = avgstudearnings < (mean(avgstudearnings) - sd(avgstudearnings)), lowcost = costofattend < (mean(costofattend) - sd(costofattend)))%>%
  mutate(avgearning = avgstudearnings < (mean(avgstudearnings) + sd(avgstudearnings)) & avgstudearnings >   (mean(avgstudearnings) - sd(avgstudearnings)))%>%
  mutate(postScorecards = dateranked >= ymd('2015-09-01')) %>%
  mutate(monthinyear = month(dateranked)) %>%
  mutate(daysfrominitial = dateranked - ymd('2013-03-24')) 
```

##Exploratory Analysis

Initial Aggregates

As the first step of my exploratory analysis I aggregated the data set at the school and month level, the school, month, and cost level,  and the school, month, and earning level. I wanted to do this in order to support high level graphing to help understand the functional form of the dataset.

```{r}
schoolmo <- joineddata %>% group_by(schname,datemonth) %>%summarize(monthchange = sum(standardindexschool))
schoolmocost <- joineddata %>% group_by(schname,datemonth, highcost) %>%summarize(monthchange = sum(standardindexschool))
schoolmoearnings <- joineddata %>% group_by(schname,datemonth, highearning) %>%summarize(monthchange = sum(standardindexschool))
```

Initial Exploratory Graphs

The first graph I investigate is a simple scatter plot with X as the aggregated monthly change index. From the top level it is clear that there is a downward trend with lower google trends search traffic as time goes on. This may be correlated with an overall reduction in college attendance over the past decade. In addition there is clear seasonality present within the data set, with regular peaks and valleys present.

I duplicate the top level graph for both high cost schools and high earning schools to see if I can observe any clear patterns. To assist with this I separate out the high cost/earning schools from the non high cost/earning schools to see if either value trends drastically different from the overall combined data set. From a birds eye view there does not seem to be any meaningful difference in slope for high-cost versus non high-cost schools suggesting it will not have very high explanatory power. Interestingly it does look like high earning schools trend downward slightly harder than non high earning schools which would be contrary to expectation. 

```{r}
#Plotting data to observe overall trends
ggplot(schoolmo, aes(datemonth,monthchange)) + geom_point() + stat_smooth(method = "lm", col = "red")

#Trends separated into high cost schools and low cost
ggplot(schoolmocost, aes(datemonth, monthchange, color = highcost)) + geom_point() 
ggplot(schoolmocost, aes(datemonth, monthchange, color = highcost)) + geom_point() + facet_grid(rows = vars(highcost)) + stat_smooth(method = "lm", col = "black")

#Trends separated into high earning schools and low earning schools
ggplot(schoolmoearnings,aes(datemonth, monthchange, color = highearning)) + geom_point()
ggplot(schoolmoearnings, aes(datemonth, monthchange, color = highearning)) + geom_point() + facet_grid(rows = vars(highearning)) + stat_smooth(method = "lm", col = "black")
```

##Regression Analysis

Upon performing my regression analysis I first investigated the primary relationship I was interested in exploring, the interaction of the scorecard treatment and high earning schools on google trends patterns. What I found was a very rough estimation that from a baseline value of .0323 standard deviation increase in search results, post scorecard going into effect you would expect to see a reduction in search results of -.1715 standard deviations in search index for non high earning schools mitigated to a reduction of -.1115 standard deviations for high earning schools. This is obviously missing quite a bit but I wanted to take a look at the dataset from a high level to have a baseline perspective on the interaction term of note.

In regression 2, 3 and 4 I step by step layer in additional variables of interest. In regression 2 I layer in school cost and find no significant effect, therefore I leave it out of future steps. In regression 3 I add in a control for time effects and find that as expected the further into the future we go, the more we expect our value to decrease from our intercept. Finally in regression 4 I add in a factor variable to control for the seasonality impacts.

After controlling for seasonality and time series effects I find that for non high earning schools, the scorecard increased the average search for a school by .0630 standard deviations, however for high earning schools that increase was much higher at .1231 standard deviations. The scorecard had a overall net positive effect on searches for colleges irregardless of their earning potential but it had a GREATER effect for high earning schools. Of note each and every variable within my final regression was statistically significant to the .001 level. After controlling for the overall decreasing interest in universities (as represented by google trends results) I was able to get to the actual relationship of interest.

As a followup of note I also investigated the impact of low earning schools and medium earning schools to confirm my results and found that as expected low earning schools and medium earning schools both received around a .6-7 standard deviation increase in their search results, when you observed the the interaction between low/medium earning status and the collegescorecard treatment variable.

```{r}

#Initial Top Level Regression of standardindex by school explained by the interaction of scorecard treatment and whether a school is high earning or not. This purposefully leaves out the downward trend and seasonality just to get an inital grasp of the data set
reg1 <- feols(standardindexschool ~ postScorecards + highearning + postScorecards*highearning,data = joineddata)
etable(reg1)

#Adding whether a school is high-cost or not to the regression. As expected from initial analysis of scatterplots above, the cost of a school has a very marginal effect on the increase or decrease of popularity in trends data over this investigative time period.
reg2 <- feols(standardindexschool ~ postScorecards + highearning + postScorecards*highearning + highcost, data = joineddata)
etable(reg2)

#Adding a calculated variable based on the initial date of trends data pulled in order to create a linear variable to separate the time effect.
reg3 <- feols(standardindexschool ~ daysfrominitial + postScorecards + highearning + postScorecards *highearning, data = joineddata)
etable(reg3)

#Additionally adding a factor for months to also remove the seasonality from the dataset to better explain the relationship.
reg4 <- feols(standardindexschool ~ daysfrominitial + postScorecards + highearning + postScorecards*highearning + i(monthinyear), data = joineddata)
etable(reg4)

#Final regressions to compare the exact opposite effects, observing the effects of low earning schools and low cost schools, as well as the impact of average earning schools.
reg5 <- feols(standardindexschool ~ daysfrominitial + postScorecards + lowearning + lowcost + postScorecards*lowearning + i(monthinyear), data = joineddata)
etable(reg5)

reg6 <- feols(standardindexschool ~ daysfrominitial + postScorecards + avgearning + postScorecards*avgearning + i(monthinyear), data = joineddata)
etable(reg6)
```

##Final Conclusions

After performing the regression analysis I can confidentially say that the college scorecard was effective in increasing interest in primarily bachelor providing schools (as measured by google trends results) and I can additionally say that while all school saw a positive effect from the college scorecard, it was focused upon those schools who's graduates end up commanding a higher average salary upon leaving their institution as observable from the much higher coefficient on the interaction of high earning potential school and the college scorecard treatment when compared to the interaction of low and medium earning potential schools and the college scorecard. While I will admit that upon first glance I was skeptical and assumed that people just went to whatever school was both nearby and had a decent reputation, there is evidence to support the fact that college applicants are a more informed and logical group than I first assumed.
